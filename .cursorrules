# MASTER RULES: See petrosa_k8s/.cursorrules

**IMPORTANT**: This file contains service-specific rules only. For ecosystem-wide rules, architecture, shared resources, deployment patterns, and cross-service integration, always refer to:
- **Master Cursorrules**: `/Users/yurisa2/petrosa/petrosa_k8s/.cursorrules`

When working on this service, read the master cursorrules first to understand the full system context.

---

## Socket Client - Satellite Rules

### Service Context

The **Petrosa Socket Client** is a high-performance WebSocket client that maintains a persistent connection to the Binance WebSocket API for real-time market data streaming. It receives trades, tickers, and order book depth updates and publishes them to NATS for consumption by downstream services (realtime-strategies, data-manager).

This service operates **24/7 with a single replica deployment** because it maintains stateful WebSocket connections. Multiple replicas would create duplicate data streams and unnecessary load on the Binance API. **Scaling must be vertical only** (increase CPU/memory resources), never horizontal (adding replicas).

The socket-client is a critical component in the data ingestion pipeline, processing **1000+ messages per second** during peak trading hours and ensuring zero message loss through circuit breakers, exponential backoff, and queue management.

### Cross-References to Master

- **Ecosystem Architecture**: See master § System Architecture Overview → Data Ingestion & Storage Layer
- **Work Tracking**: See master § Centralized Work Tracking with GitHub Projects
- **NATS Topics**: See master § NATS Message Bus → `binance.websocket.data`
- **Deployment Patterns**: See master § Socket Client Configuration
- **Scaling Strategy**: See master § Deployment Patterns (WHY single replica is critical)
- **Shared Resources**: See master § Shared Resources (ConfigMaps, Secrets)

---

## Service-Specific Rules

### Repository Structure

**Key Files**:
- `README.md` - Comprehensive service documentation
- `Makefile` - Development commands
- `k8s/deployment.yaml` - **IMPORTANT: Single replica only**
- `socket_client/core/client.py` - Main WebSocket client
- `socket_client/core/nats_publisher.py` - NATS publishing logic
- `socket_client/utils/circuit_breaker.py` - Circuit breaker implementation
- `socket_client/models/message.py` - Message transformation models

**Directory Layout**:
```
petrosa-socket-client/
├── socket_client/
│   ├── core/           # Core WebSocket and NATS logic
│   │   ├── client.py
│   │   ├── nats_publisher.py
│   │   └── reconnection.py
│   ├── models/         # Message models
│   │   └── message.py
│   ├── utils/          # Utilities
│   │   ├── circuit_breaker.py
│   │   ├── logger.py
│   │   └── metrics.py
│   └── health/         # Health check server
│       └── server.py
└── k8s/                # Kubernetes manifests
    ├── deployment.yaml
    ├── service.yaml
    └── configmap.yaml
```

### CRITICAL: Single Replica Deployment

**Why Single Replica is Mandatory**:

1. **Stateful Connection**: WebSocket maintains state (subscriptions, connection ID)
2. **Duplicate Data**: Multiple replicas = multiple identical WebSocket connections = duplicate messages to NATS
3. **API Load**: Each replica creates separate connection to Binance, multiplying load unnecessarily
4. **Message Ordering**: Multiple streams can cause race conditions in downstream consumers
5. **Cost**: Unnecessary resource usage and Binance API quota consumption

**Correct Deployment Pattern**:
```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 1  # NEVER change this to > 1
  template:
    spec:
      containers:
      - name: socket-client
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "2Gi"    # Scale UP these values
            cpu: "1000m"      # not replica count
```

**Vertical Scaling Strategy**:
```yaml
# If more resources needed, increase these values:
resources:
  limits:
    memory: "4Gi"    # Increase memory
    cpu: "2000m"      # Increase CPU
    
# NEVER do this:
# replicas: 3  ❌ WRONG - creates duplicate streams
```

### WebSocket Connection Management

**Connection Lifecycle**:
```python
class BinanceWebSocketClient:
    """
    Lifecycle:
    1. Connect to wss://stream.binance.com:9443
    2. Subscribe to configured streams
    3. Start message processing loop
    4. Start ping/pong keepalive loop
    5. Start heartbeat monitoring loop
    6. Handle reconnections with exponential backoff
    """
    
    async def start(self):
        await self._connect_nats()
        await self._connect_websocket()
        self.processor_task = asyncio.create_task(self._process_messages())
        self.ping_task = asyncio.create_task(self._ping_loop())
        self.heartbeat_task = asyncio.create_task(self._heartbeat_loop())
    
    async def _connect_websocket(self):
        """Connect with exponential backoff."""
        attempt = 0
        while attempt < self.max_reconnect_attempts:
            try:
                self.websocket = await websockets.connect(
                    self.ws_url,
                    ping_interval=30,
                    ping_timeout=10,
                    close_timeout=10
                )
                await self._subscribe_to_streams()
                self.is_connected = True
                return
            except Exception as e:
                attempt += 1
                delay = min(60, self.base_delay * (2 ** attempt))
                logger.warning(f"Connection failed, retry {attempt}/{self.max_reconnect_attempts} in {delay}s")
                await asyncio.sleep(delay)
```

**Exponential Backoff Pattern**:
```python
def calculate_backoff_delay(attempt: int, base_delay: int = 5, max_delay: int = 60) -> float:
    """
    Calculate exponential backoff with jitter.
    
    Formula: min(max_delay, base_delay * 2^attempt + random(0, 1))
    
    Examples:
    - Attempt 1: 5s + jitter
    - Attempt 2: 10s + jitter
    - Attempt 3: 20s + jitter
    - Attempt 4: 40s + jitter
    - Attempt 5+: 60s + jitter (capped)
    """
    import random
    delay = base_delay * (2 ** attempt)
    delay = min(delay, max_delay)
    jitter = random.uniform(0, 1)
    return delay + jitter
```

**Circuit Breaker Integration**:
```python
# Protect against cascading failures
websocket_breaker = CircuitBreaker(
    failure_threshold=5,      # Open after 5 consecutive failures
    recovery_timeout=60,      # Try recovery after 60 seconds
    expected_exception=websockets.exceptions.WebSocketException
)

nats_breaker = CircuitBreaker(
    failure_threshold=5,
    recovery_timeout=60,
    expected_exception=nats.errors.Error
)
```

### Stream Configuration

**Supported Stream Types**:
```python
# Trade streams (individual trades)
STREAM_PATTERN: "{symbol}@trade"
Example: "btcusdt@trade"

# Ticker streams (24hr statistics)
STREAM_PATTERN: "{symbol}@ticker"
Example: "btcusdt@ticker"

# Depth streams (order book top 20 levels, 100ms updates)
STREAM_PATTERN: "{symbol}@depth20@100ms"
Example: "btcusdt@depth20@100ms"

# Aggregated trades
STREAM_PATTERN: "{symbol}@aggTrade"
Example: "btcusdt@aggTrade"
```

**Subscription Message**:
```json
{
  "method": "SUBSCRIBE",
  "params": [
    "btcusdt@trade",
    "btcusdt@ticker",
    "btcusdt@depth20@100ms"
  ],
  "id": 1
}
```

**Configuration via Environment**:
```bash
# Comma-separated stream list
BINANCE_STREAMS="btcusdt@trade,btcusdt@ticker,btcusdt@depth20@100ms,ethusdt@trade,ethusdt@ticker"

# WebSocket URL
BINANCE_WS_URL="wss://stream.binance.com:9443"

# NATS configuration
NATS_URL="nats://nats-server.nats:4222"
NATS_TOPIC="binance.websocket.data"
```

### Message Processing Pipeline

**Message Flow**:
```
WebSocket → Queue (5000 capacity) → Processor → Transform → NATS Publisher
    ↓            ↓                        ↓           ↓            ↓
 Parse JSON   Backpressure            Validate    Add UUID    Publish
              Detection                           Add Metadata  Topic
```

**Message Transformation**:
```python
# Input from Binance
{
  "e": "trade",
  "E": 1234567890123,
  "s": "BTCUSDT",
  "t": 12345,
  "p": "50000.00",
  "q": "0.001"
}

# Output to NATS
{
  "stream": "btcusdt@trade",
  "data": { /* original Binance data */ },
  "timestamp": "2024-01-01T00:00:00.000Z",
  "message_id": "550e8400-e29b-41d4-a716-446655440000",
  "source": "binance-websocket",
  "version": "1.0"
}
```

**Queue Management**:
```python
# Message queue with backpressure detection
message_queue = asyncio.Queue(maxsize=5000)

async def _handle_message(self, message: str):
    try:
        # Check queue capacity
        queue_utilization = self.message_queue.qsize() / 5000
        if queue_utilization > 0.9:
            logger.warning(f"Queue at {queue_utilization*100}% capacity")
            self.dropped_messages += 1
            return  # Drop message to prevent queue overflow
        
        # Add to queue for processing
        await self.message_queue.put(message)
    except asyncio.QueueFull:
        self.dropped_messages += 1
        logger.error("Queue full, dropping message")
```

### Health Checks and Monitoring

**Health Endpoints**:
```python
@app.get("/healthz")
async def healthz():
    """Liveness probe - is process running?"""
    return {"status": "healthy"}

@app.get("/ready")
async def ready():
    """Readiness probe - can handle traffic?"""
    if not client.is_connected or client.nats_client.is_closed:
        return {"status": "not_ready"}
    return {"status": "ready"}

@app.get("/metrics")
async def metrics():
    """Prometheus metrics."""
    return {
        "messages_processed": client.processed_messages,
        "messages_dropped": client.dropped_messages,
        "queue_size": client.message_queue.qsize(),
        "websocket_connected": client.is_connected,
        "nats_connected": not client.nats_client.is_closed
    }
```

**Heartbeat Monitoring** (every 30 seconds):
```json
{
  "level": "INFO",
  "message": "HEARTBEAT: WebSocket Client Statistics",
  "connection_status": true,
  "messages_processed_since_last": 1250,
  "messages_per_second": 41.67,
  "total_processed": 150000,
  "queue_utilization_percent": 0.9,
  "reconnect_attempts": 0
}
```

### Common Issues & Solutions

#### 1. High Message Drop Rate

**Symptom**: `messages_dropped > 0` or `queue_utilization > 90%`

**Solutions**:
```yaml
# Increase queue size
env:
- name: MAX_QUEUE_SIZE
  value: "10000"

# Increase resources (vertical scaling)
resources:
  limits:
    memory: "4Gi"
    cpu: "2000m"
```

#### 2. Frequent Reconnections

**Symptom**: WebSocket disconnects every few minutes

**Check**:
```bash
# View connection logs
kubectl --kubeconfig=k8s/kubeconfig.yaml logs -n petrosa-apps -l app=socket-client | grep "reconnect"

# Check network policies
kubectl --kubeconfig=k8s/kubeconfig.yaml get networkpolicies -n petrosa-apps

# Verify egress to Binance
kubectl --kubeconfig=k8s/kubeconfig.yaml exec -it deployment/petrosa-socket-client -n petrosa-apps -- nc -zv stream.binance.com 9443
```

**Solutions**:
```python
# Increase ping interval
ping_interval=60  # from 30

# Adjust timeout values
close_timeout=20  # from 10
```

#### 3. NATS Publishing Failures

**Symptom**: `nats_state: "disconnected"` or publish errors

**Solutions**:
```bash
# Check NATS connectivity
kubectl --kubeconfig=k8s/kubeconfig.yaml exec -it deployment/petrosa-socket-client -n petrosa-apps -- nc -zv nats-server.nats 4222

# Verify NATS server status
kubectl --kubeconfig=k8s/kubeconfig.yaml get pods -n nats

# Check circuit breaker state
curl http://petrosa-socket-client/metrics | jq '.circuit_breaker'
```

#### 4. Memory Leaks

**Symptom**: Gradual memory increase, eventual OOM kill

**Debug**:
```python
# Enable memory profiling
import tracemalloc
tracemalloc.start()

# Periodic snapshot
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')
for stat in top_stats[:10]:
    print(stat)
```

**Common Causes**:
- Queue not being drained (processor task crashed)
- Websocket messages not being garbage collected
- NATS connection leaking

### Development Workflow

**Local Development**:
```bash
# Setup
make setup

# Run with local NATS (requires NATS server running)
make run-local

# Dry run (connect but don't publish)
NATS_DRY_RUN=true python -m socket_client.main
```

**Testing**:
```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests (requires test NATS)
pytest tests/integration/ -v

# Load test
pytest tests/load/test_high_throughput.py -v
```

**Debugging**:
```bash
# Check WebSocket connection
kubectl --kubeconfig=k8s/kubeconfig.yaml logs -n petrosa-apps -l app=socket-client --tail=100 | grep "Connected to Binance"

# Monitor message flow
kubectl --kubeconfig=k8s/kubeconfig.yaml exec -it deployment/petrosa-socket-client -n petrosa-apps -- curl localhost:8080/metrics

# Check queue status
watch -n 1 'kubectl --kubeconfig=k8s/kubeconfig.yaml exec deployment/petrosa-socket-client -n petrosa-apps -- curl -s localhost:8080/metrics | jq ".queue_size"'
```

### Configuration

**Environment Variables** (service-specific):
```bash
# WebSocket configuration
BINANCE_WS_URL="wss://stream.binance.com:9443"
BINANCE_STREAMS="btcusdt@trade,btcusdt@ticker,btcusdt@depth20@100ms"

# Reconnection settings
WEBSOCKET_RECONNECT_DELAY=5          # Base delay in seconds
WEBSOCKET_MAX_RECONNECT_ATTEMPTS=10  # Max attempts before giving up
WEBSOCKET_PING_INTERVAL=30           # Ping interval in seconds

# Queue management
MAX_QUEUE_SIZE=5000                  # Message queue capacity
MESSAGE_TTL_SECONDS=60               # Drop messages older than this

# NATS configuration
NATS_TOPIC="binance.websocket.data"  # Topic for publishing

# Monitoring
ENABLE_HEARTBEAT=true                # Enable periodic heartbeat logs
HEARTBEAT_INTERVAL=30                # Heartbeat interval in seconds
```

### Success Metrics

**Performance Targets**:
- Message processing rate: 1000+ messages/second
- Message drop rate: < 0.1%
- Queue utilization: < 80% (average)
- Reconnection frequency: < 1 per hour
- Time since last message: < 1 second

**Monitoring**:
```bash
# Check processing rate
kubectl --kubeconfig=k8s/kubeconfig.yaml logs -n petrosa-apps -l app=socket-client | grep "messages_per_second"

# Verify data flow to NATS
kubectl --kubeconfig=k8s/kubeconfig.yaml exec -it deployment/petrosa-realtime-strategies -n petrosa-apps -- nats sub binance.websocket.data
```

### Always Reference

- For ecosystem-wide rules and shared patterns: **See master cursorrules**
- For work tracking and issue creation: **See master § Centralized Work Tracking with GitHub Projects**
- For NATS topic structure and consumers: **See master § NATS Message Bus**
- For deployment and scaling guidelines: **See master § Socket Client Configuration**
- **CRITICAL**: For why horizontal scaling is forbidden: **See master § Deployment Patterns → Socket Client**
