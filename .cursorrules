# Cursor AI Rules for Petrosa Systems

## Repository Context
This is a unified configuration for all Petrosa systems:
- **petrosa-bot-ta-analysis**: Technical Analysis bot for crypto trading
- **petrosa-tradeengine**: Cryptocurrency trading engine system
- **petrosa-binance-data-extractor**: Cryptocurrency data extraction system
- **petrosa-socket-client**: Binance WebSocket client for real-time data streaming

All systems use the same remote MicroK8s cluster, CI/CD pipeline patterns, and development workflows.

## Key Files to Reference
- `README.md` - Project-specific documentation and usage guide
- `Makefile` - Complete command reference for all development tasks
- `k8s/` - Kubernetes manifests for production deployment
- `k8s/kubeconfig.yaml` - Remote MicroK8s cluster configuration
- `scripts/` - Automation scripts for setup, testing, and deployment
- `docs/` - Comprehensive documentation and guides

## Prerequisites & Installation
- **Python 3.11+**: Required for development and runtime
- **Docker**: Required for containerization and local testing
- **kubectl**: Required for Kubernetes deployment (remote cluster)
- **Make**: Required for using the Makefile commands

**Note**: All projects use a **remote MicroK8s cluster** - no local Kubernetes installation required.

## Quick Start Commands
```bash
# Complete setup
make setup

# Run local pipeline
make pipeline

# Deploy to Kubernetes
make deploy

# Check deployment status
make k8s-status
```

## Common Issues & Solutions

### 1. Python Environment Issues
```bash
# Check Python version
python3 --version

# Recreate virtual environment
rm -rf .venv
make setup
```

### 2. Docker Build Issues
```bash
# Clean Docker cache
make docker-clean

# Rebuild image
make build
```

### 3. Kubernetes Connection Issues
```bash
# Set kubeconfig for remote cluster
export KUBECONFIG=k8s/kubeconfig.yaml

# Check cluster connection
kubectl --kubeconfig=k8s/kubeconfig.yaml cluster-info

# Check namespace
kubectl --kubeconfig=k8s/kubeconfig.yaml get namespace petrosa-apps

# View deployment status
make k8s-status

# Note: This is a remote MicroK8s cluster - no local installation needed
```

### 4. WebSocket Connection Issues
```bash
# Check WebSocket connectivity
python -c "import websockets; print('WebSocket library available')"

# Test NATS connection
python -c "import nats; print('NATS library available')"

# Check environment variables
env | grep -E "(BINANCE|NATS|LOG)"
```

## Development Workflow

### 1. Initial Setup
```bash
# Complete environment setup
make setup

# Install development dependencies
make install-dev
```

### 2. Code Quality Checks
```bash
# Run all linting
make lint

# Format code
make format

# Run tests
make test

# Security scan
make security
```

### 3. Docker Operations
```bash
# Build image
make build

# Test container
make container

# Run in Docker
make run-docker
```

### 4. Kubernetes Deployment
```bash
# Set kubeconfig for remote cluster
export KUBECONFIG=k8s/kubeconfig.yaml

# Deploy to remote cluster
make deploy

# Check status
make k8s-status

# View logs
make k8s-logs

# Clean up
make k8s-clean
```

## Kubernetes Configuration
## CRITICAL: ALWAYS FOLLOW THESE RULES
- **AVOID INTERACTIVE CLI COMMANDS AT ALL COSTS**: Never suggest or use interactive commands that require user input, confirmation, or manual intervention. All commands must be non-interactive and automated.
- **BEFORE making any changes**: Check `docs/REPOSITORY_SETUP_GUIDE.md` and `docs/QUICK_REFERENCE.md`
- **WHEN suggesting kubectl commands**: Always include `--kubeconfig=k8s/kubeconfig.yaml`
- **WHEN dealing with credentials**: ONLY use existing secret `petrosa-sensitive-credentials`
- **WHEN dealing with configuration**: Use `petrosa-common-config` for general settings and project-specific configmaps
- **WHEN running GitHub CLI**: ALWAYS use file-based approach: `gh command > /tmp/file.json && cat /tmp/file.json`
- **WHEN fixing CI/CD issues**: Continue until GitHub Actions pipeline passes
- **WHEN writing Python code**: Follow PEP 8, use type hints, add proper error handling
- **NEVER suggest AWS EKS commands** - this is a MicroK8s setup
- **NEVER create new Kubernetes secrets/configmaps** - use existing ones only
- **NEVER replace VERSION_PLACEHOLDER in Kubernetes manifests** - it's part of the deployment system

### Remote Cluster Setup
- **Cluster Type**: Remote MicroK8s (no local installation required)
- **Connection**: Use `k8s/kubeconfig.yaml` for cluster access
- **Server**: Remote MicroK8s cluster at `https://192.168.194.253:16443`

### Namespace
- **Name**: `petrosa-apps`
- **Labels**: `app=petrosa-socket-client`

### Components
- **Deployment**: 3 replicas with health checks
- **Service**: ClusterIP on port 80
- **Ingress**: SSL-enabled with Let's Encrypt
- **HPA**: Auto-scaling based on CPU/memory

## Environment Variables

### Common Variables (All Projects)
- `ENVIRONMENT`: Environment name (default: production)
- `LOG_LEVEL`: Logging level (default: INFO)

### Socket Client Specific Variables
- `BINANCE_WS_URL`: Binance WebSocket URL (default: wss://stream.binance.com:9443)
- `BINANCE_STREAMS`: Comma-separated list of streams to subscribe to
- `NATS_URL`: NATS server URL
- `NATS_TOPIC`: NATS topic for publishing messages
- `WEBSOCKET_RECONNECT_DELAY`: Reconnection delay in seconds (default: 5)
- `WEBSOCKET_MAX_RECONNECT_ATTEMPTS`: Maximum reconnection attempts (default: 10)
- `MESSAGE_TTL_SECONDS`: Message TTL in seconds (default: 60)

## Testing Procedures

### Standard Test Commands
```bash
# Run all tests
make test

# Run with coverage
python -m pytest tests/ -v --cov=. --cov-report=term

# Run specific test file
python -m pytest tests/test_specific.py -v

# Run linting
make lint

# Run security scan
make security
```

### Pipeline Testing
```bash
# Run complete local pipeline
make pipeline

# Run specific pipeline stages
./scripts/local-pipeline.sh lint test
./scripts/local-pipeline.sh build container
./scripts/local-pipeline.sh deploy
```

## Troubleshooting Scripts

### Complete Diagnostics
```bash
# Run all checks
./scripts/local-pipeline.sh all

# Quick fixes
make setup
make test
```

### Specific Component Checks
```bash
# Check Python environment
./scripts/local-pipeline.sh setup

# Check dependencies
./scripts/local-pipeline.sh lint

# Check Docker
./scripts/local-pipeline.sh build

# Check Kubernetes
./scripts/local-pipeline.sh deploy
```

## Local Pipeline

### Complete Pipeline
```bash
# Run all stages
./scripts/local-pipeline.sh all

# Run specific stages
./scripts/local-pipeline.sh lint test
./scripts/local-pipeline.sh build container
./scripts/local-pipeline.sh deploy
```

### Pipeline Stages
1. **Setup**: Environment and dependencies
2. **Lint**: Code quality checks (flake8, black, ruff, mypy)
3. **Test**: Unit tests with coverage
4. **Security**: Vulnerability scanning with Trivy
5. **Build**: Docker image building
6. **Container**: Container testing
7. **Deploy**: Kubernetes deployment

## GitHub CLI Commands

### File Output Pattern
When running GitHub CLI (`gh`) commands, always dump output to a temporary file and then read it:
```bash
# Example pattern
gh api repos/owner/repo/contents/path > /tmp/gh_output.json
cat /tmp/gh_output.json

# For commands that need processing
gh repo list --json name,url > /tmp/repos.json
jq -r '.[].name' /tmp/repos.json

# Clean up after use
rm /tmp/gh_output.json
```

### Common GitHub CLI Patterns
```bash
# Get repository info
gh api repos/owner/repo > /tmp/repo_info.json

# List issues
gh issue list --repo owner/repo --json number,title,state > /tmp/issues.json

# Get workflow runs
gh run list --repo owner/repo --json id,status,conclusion > /tmp/runs.json

# Read and process the output
cat /tmp/repo_info.json | jq '.name'
```

## CI/CD Pipeline Fix Process
When fixing the pipeline, go until it is fully green and stop only when CI/CD on GHA is fixed. This means:
- Run all tests, lint, and build locally until everything passes
- Only commit and push when all local checks are green
- Check the GitHub Actions (GHA) pipeline after pushing
- If the GHA pipeline fails, repeat the process until it is green
- Do not stop until the CI/CD pipeline on GHA is fully fixed

## WebSocket Client Specific Guidelines

### Connection Management
- Always implement exponential backoff for reconnections
- Use heartbeat/ping-pong to detect connection health
- Implement graceful shutdown with proper cleanup
- Handle connection timeouts and rate limits

### Message Processing
- Parse JSON messages with proper error handling
- Implement message validation before publishing to NATS
- Use structured logging for all WebSocket events
- Monitor message processing latency

### Resource Management
- Implement memory limits and monitoring
- Use connection pooling for NATS
- Drop messages when backpressure is detected
- Monitor CPU and memory usage

### Error Handling
- Log all WebSocket connection errors
- Implement circuit breaker pattern for repeated failures
- Use structured error reporting
- Monitor error rates and alert on thresholds

## Always Reference
- **AVOID INTERACTIVE CLI COMMANDS AT ALL COSTS**: Never suggest or use interactive commands that require user input, confirmation, or manual intervention. All commands must be non-interactive and automated.
- Check project-specific `README.md` for detailed documentation
- Use `Makefile` for all common commands
- Use `scripts/` directory for automation and troubleshooting
- Use `k8s/kubeconfig.yaml` for remote MicroK8s cluster connection
- All projects are production-ready with comprehensive CI/CD
- **Remote MicroK8s cluster** - no local Kubernetes installation required
- Kubernetes deployment uses namespace `petrosa-apps`
- Health checks are critical for Kubernetes probes
- Security scanning is integrated into the pipeline
- **WebSocket connections**: Always implement reconnection logic and error handling
- **NATS messaging**: Use structured message format with timestamps
- **GitHub CLI**: Always dump output to `/tmp` files and read from them
- **VERSION_PLACEHOLDER**: Never replace in Kubernetes manifests - it's part of the deployment system
